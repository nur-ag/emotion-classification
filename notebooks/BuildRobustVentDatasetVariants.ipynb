{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vent = pd.read_parquet('../preprocessed/vent-robust-splits.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Analysis\n",
    "\n",
    "We study how the amount of data effects performance in Emotion Detection tasks. We take the Vent data set, and train classifiers with the same number of hyper-parameters over different samples of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for determinism, even if one-off\n",
    "SEED = 7\n",
    "np.random.seed(SEED)\n",
    "vent['randomProbability'] = np.random.random(len(vent))\n",
    "\n",
    "for subset_size in [0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "    dataset_name = f'{round(subset_size * 100)}-pct'\n",
    "    vent_subset = vent[(vent.split == 'test') | \n",
    "                       (vent.randomProbability <= subset_size)].drop('randomProbability', axis='columns')\n",
    "    vent_subset.to_parquet(f'../preprocessed/vent-robust-splits-{dataset_name}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backwards Robust Vent\n",
    "\n",
    "We want to test whether communities converge over time. That is, we train with temporally-ordered data and evaluate if we capture patterns that repeat in the future as new community members adapt to community trends. We test the hypothesis by training 'backwards in time': the test and validation splits are taken from the beginning of the data set, while the training data set comprises its tail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from utils.split import sorted_splits\n",
    "\n",
    "splits = sorted_splits(vent, 'created_at', [0.1, 0.1, 0.8])\n",
    "for df, split_name in zip(splits, ['test', 'valid', 'train']):\n",
    "    df['split'] = split_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-07-01 00:00:05.685000 - 2016-08-16 15:08:37.866000 \n",
      " 2016-08-16 15:08:38.151000 - 2016-10-08 12:16:25.149000 \n",
      " 2016-10-08 12:16:34.336000 - 2018-12-14 08:12:36.867000\n"
     ]
    }
   ],
   "source": [
    "print(splits[0].created_at.min(), '-',\n",
    "      splits[0].created_at.max(), '\\n',\n",
    "      splits[1].created_at.min(), '-',\n",
    "      splits[1].created_at.max(), '\\n',\n",
    "      splits[2].created_at.min(), '-',\n",
    "      splits[2].created_at.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "backwards_robust = pd.concat(splits, ignore_index=True)\n",
    "backwards_robust.to_parquet('../preprocessed/vent-robust-splits-backwards.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
